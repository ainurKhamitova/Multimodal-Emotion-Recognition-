Emotion recognition plays a crucial role in human-computer
interaction, significantly influencing the advancement of virtual
assistants, mental health diagnosis tools, and customer experience
analysis systems. Our senior project aims to develop
an advanced multimodal emotion recognition (MER) model
using modern deep learning techniques and fusion methods.
Most traditional emotion recognition models rely on a single
modality for decision-making, such as facial expressions or
text. However, this approach can be limited in capturing the
complexity of human emotions. To overcome this limitation,
we will integrate multiple input types to create a more comprehensive
model, reducing misclassifications and improving
overall system performance.
Our system includes an emotion recognition model and a
user interface for interaction. The web application will serve
as the interface, allowing users to upload video materials of
a specified duration. The application extracts audio, video,
and text from the uploaded video and feeds them into different
deep-learning models customized for each modality.
The outputs, representing probabilities for various emotion
classes (e.g., ”happy,” ”sad,” ”fearful,” ”surprised,” ”angry,”
”disgusted,” and ”neutral”), will be combined using fusion
techniques for enhanced accuracy. The web app then presents
visual representations of the emotions through graphs and
descriptions for user interpretation.
